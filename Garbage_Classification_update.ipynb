{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Garbage-Classification-update.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1F-5kcGWutJRi_eQR_JFlaVfWLD4yx-bH",
      "authorship_tag": "ABX9TyM4IjFBvh8TtiYRzAca3iBo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sovb/Garbage-Classification/blob/main/Garbage_Classification_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "HY7X7dmCC7Mu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7bsUAyDnCvd2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import cv2\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.layers import Conv2D, Flatten, MaxPooling2D,Dense,Dropout,SpatialDropout2D\n",
        "from keras.models  import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n",
        "import os\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import random,os,glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#added by sovra\n",
        "import scipy\n",
        "from scipy import *\n",
        "import skimage\n",
        "from skimage import filters\n",
        "from skimage import io \n",
        "import matplotlib.pyplot as plt  #added by sovra\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile as zf\n",
        "import shutil\n",
        "import re\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = \"/content/drive/MyDrive/Colab Notebooks/dataset-resized\"\n",
        "\n",
        "img_list = glob.glob(os.path.join(dir_path, '*/*.jpg'))\n",
        "len(img_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0SonvjwE88N",
        "outputId": "b4ed69ce-c342-4cdd-842a-afdccf371e4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2527"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "comment: maybe after performing the training, we can try gaussian blur to see if it makes a difference"
      ],
      "metadata": {
        "id": "Psarw3KlP_9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras,os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def preprocess_data():\n",
        "    # All images will be rescaled by 1./255\n",
        "    train_datagen=ImageDataGenerator(rescale=1./255)\n",
        "    val_datagen=ImageDataGenerator(rescale=1./255)\n",
        "    test_datagen=ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator=train.flow_from_directory(dir_path,#this is the target directory\n",
        "                                          target_size=(300,300),#all images resized as follows\n",
        "                                          batch_size=32,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          class_mode='categorical',#we need categorical labels\n",
        "                                          subset='training')\n",
        "   \n",
        "    validation_generator = val_datagen.flow_from_directory(dir_path,\n",
        "                                              target_size=(300,300),#all images resized as follows\n",
        "                                          batch_size=32,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          class_mode='categorical',#we need categorical labels\n",
        "                                          subset='training',\n",
        "                                          shuffle=False)\n",
        "\n",
        "    test_generator=test.flow_from_directory(dir_path,\n",
        "                                          target_size=(300,300),\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          batch_size=32,\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='validation',\n",
        "                                          shuffle=False)\n",
        "    \n",
        "    return train_generator, validation_generator, test_generator\n",
        "\n",
        "train_generator, validation_generator, test_generator = preprocess_data()\n",
        "\n",
        "\n",
        "conv_base = tf.keras.applications.vgg16.VGG16(weights=None, include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.7))\n",
        "# Since we have a binary classification problem\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizers.Adam(learning_rate=1e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=50,\n",
        "    epochs=100,\n",
        "    callbacks=[history_logger],\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=20\n",
        ")\n",
        "\n",
        "\n",
        "predictions = model.predict_generator(test_generator, steps=12)\n",
        "test_preds = predictions\n",
        "test_trues = test_generator.classes"
      ],
      "metadata": {
        "id": "PJ5VwWmpoglp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=ImageDataGenerator(horizontal_flip=True,\n",
        "                         vertical_flip=True,\n",
        "                         validation_split=0.2,#changed from 0.1\n",
        "                         rescale=1./255,\n",
        "                         shear_range = 0.1,\n",
        "                         zoom_range = 0.1,\n",
        "                         width_shift_range = 0.1,\n",
        "                         height_shift_range = 0.1,)\n",
        "\n",
        "test=ImageDataGenerator(rescale=1./255,\n",
        "                        validation_split=0.1)\n",
        "\n",
        "train_generator=train.flow_from_directory(dir_path,\n",
        "                                          target_size=(300,300),\n",
        "                                          batch_size=32,\n",
        "                                          color_mode=\"rgb\",\n",
        "                                          class_mode='categorical',\n",
        "                                          subset='training')\n",
        "\n",
        "test_generator=test.flow_from_directory(dir_path,\n",
        "                                        target_size=(300,300),\n",
        "                                        batch_size=32,\n",
        "                                        color_mode=\"rgb\",\n",
        "                                        class_mode='categorical',\n",
        "                                        subset='validation')\n",
        "\n",
        "\n",
        "\n",
        "labels = (train_generator.class_indices)\n",
        "print(labels)\n",
        "labels = dict((v,k) for k,v in labels.items())\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoywIMM8Hk1F",
        "outputId": "20013beb-3ca0-4a5b-a63a-982f0efaa291"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2024 images belonging to 6 classes.\n",
            "Found 251 images belonging to 6 classes.\n",
            "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n",
            "{0: 'cardboard', 1: 'glass', 2: 'metal', 3: 'paper', 4: 'plastic', 5: 'trash'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, label_batch in train_generator:\n",
        "  break\n",
        "image_batch.shape, label_batch.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daRnf-l4q2jw",
        "outputId": "815da124-4550-4432-cb0b-434dedbf2f97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 300, 300, 3), (32, 6))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (train_generator.class_indices)\n",
        "\n",
        "Labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(Labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxk3zB-i2LQU",
        "outputId": "c5d37d58-6cb4-4dab-a231-d89e4902d81c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cardboard': 0, 'glass': 1, 'metal': 2, 'paper': 3, 'plastic': 4, 'trash': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "#Convolution blocks\n",
        "\n",
        "model.add(Conv2D(32,(3,3), padding='same',input_shape=(300,300,3),activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2)) \n",
        "#model.add(SpatialDropout2D(0.5)) # No accuracy\n",
        "\n",
        "model.add(Conv2D(64,(3,3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2)) \n",
        "#model.add(SpatialDropout2D(0.5))\n",
        "\n",
        "model.add(Conv2D(32,(3,3), padding='same',activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2)) \n",
        "\n",
        "#Classification layers\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "#model.add(SpatialDropout2D(0.5))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32,activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6,activation='softmax'))\n",
        "\n",
        "filepath=\"trained_model.woo\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]"
      ],
      "metadata": {
        "id": "XNUv6oY92Tjx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "W9jAAeyq2X4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']) # RMS PROP - No accuracy\n",
        "\n",
        "#es=EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)"
      ],
      "metadata": {
        "id": "wRsX_K1u5msl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=100,\n",
        "                              steps_per_epoch=2276//32,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=251//32,\n",
        "                              workers = 4,\n",
        "                              callbacks=callbacks_list) \n",
        "#41 epoch - 75% #73- 76.9%\n",
        "#78 epoch - 80%"
      ],
      "metadata": {
        "id": "JMQzGqlw5nLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "img_path = '../input/garbage classification/Garbage classification/plastic/plastic75.jpg'\n",
        "\n",
        "img = image.load_img(img_path, target_size=(300, 300))\n",
        "img = image.img_to_array(img, dtype=np.uint8)\n",
        "img=np.array(img)/255.0\n",
        "\n",
        "plt.title(\"Loaded Image\")\n",
        "plt.axis('off')\n",
        "plt.imshow(img.squeeze())\n",
        "\n",
        "p=model.predict(img[np.newaxis, ...])\n",
        "\n",
        "#print(\"Predicted shape\",p.shape)\n",
        "print(\"Maximum Probability: \",np.max(p[0], axis=-1))\n",
        "predicted_class = labels[np.argmax(p[0], axis=-1)]\n",
        "print(\"Classified:\",predicted_class)"
      ],
      "metadata": {
        "id": "Q8YSY9m_50FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes=[]\n",
        "prob=[]\n",
        "print(\"\\n-------------------Individual Probability--------------------------------\\n\")\n",
        "\n",
        "for i,j in enumerate (p[0],0):\n",
        "    print(labels[i].upper(),':',round(j*100,2),'%')\n",
        "    classes.append(labels[i])\n",
        "    prob.append(round(j*100,2))\n",
        "    \n",
        "def plot_bar_x():\n",
        "    # this is for plotting purpose\n",
        "    index = np.arange(len(classes))\n",
        "    plt.bar(index, prob)\n",
        "    plt.xlabel('Labels', fontsize=12)\n",
        "    plt.ylabel('Probability', fontsize=12)\n",
        "    plt.xticks(index, classes, fontsize=12, rotation=20)\n",
        "    plt.title('Probability for loaded image')\n",
        "    plt.show()\n",
        "plot_bar_x()"
      ],
      "metadata": {
        "id": "67pcueaL6Cp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# ________________ Graph 1 -------------------------\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# ________________ Graph 2 -------------------------\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,max(plt.ylim())])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4-ZIg0gA6HH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "file = \"Garbage.h5\"\n",
        "keras.models.save_model(model,file)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model_file(file)\n",
        "tflite_model=converter.convert()\n",
        "open(\"garbage.tflite\",'wb').write(tflite_model)\n"
      ],
      "metadata": {
        "id": "h4biIOy56UtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLinks\n",
        "FileLinks('.')"
      ],
      "metadata": {
        "id": "6CAPJHeS6Z_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}