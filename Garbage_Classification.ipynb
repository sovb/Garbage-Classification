{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Garbage-Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qOR0OwqSHxiTYmNfDgSaAJy1AzjIDgdI",
      "authorship_tag": "ABX9TyMI5tgHzB0NJQrQlzCTWdgj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sovb/Garbage-Classification/blob/main/Garbage_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will begin by enabling autoreload so that we can \"reimport\" a module without having to restart Python completely. This is becasue when we run the first time the module will be in the sys.modlue dictionary and the interpreter will not import it again if it is already present there.\n",
        "As for matplotlib inline, with this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document."
      ],
      "metadata": {
        "id": "M9XO1eYnEQVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zhjtjByUoHU0"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision import *\n",
        "from fastai.metrics import error_rate\n",
        "from pathlib import Path\n",
        "from glob2 import glob\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile as zf\n",
        "import shutil\n",
        "import re\n",
        "import seaborn as sns\n",
        "import random"
      ],
      "metadata": {
        "id": "Lg3122dSL9u1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Data from TrashNet dataset zip file**\n",
        "https://github.com/garythung/trashnet/blob/master/data/dataset-resized.zip"
      ],
      "metadata": {
        "id": "Fp4cj9xaQ6m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = zf.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/dataset-resized.zip\",'r')\n",
        "\n",
        "files.extractall()\n",
        "files.close()"
      ],
      "metadata": {
        "id": "9FveATi3Ozr2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(os.path.join(os.getcwd(),\"dataset-resized\"))"
      ],
      "metadata": {
        "id": "t4lpe_uYRTkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41f7693-d573-477f-fe2c-a0ae52960278"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['paper', 'glass', 'metal', '.DS_Store', 'cardboard', 'plastic', 'trash']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Organize images into different folders**\n",
        "1.   Cardboard\n",
        "2.   Glass\n",
        "3.   Paper\n",
        "4.   Metal\n",
        "5.   Plastic\n",
        "6.   Trash\n",
        "\n",
        "Train, validation and test ratio being used here will be 50-25-25 however, we can modify this to 70-15-15 later on and see if there is a difference. "
      ],
      "metadata": {
        "id": "lT0aGKSARZ4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## splits indices for a folder into train, validation, and test indices with random sampling\n",
        "    ## input: folder path\n",
        "    ## output: train, valid, and test indices    \n",
        "def split_indices(folder,seed1,seed2):    \n",
        "    n = len(os.listdir(folder))\n",
        "    full_set = list(range(1,n+1))\n",
        "\n",
        "    ## train indices\n",
        "    random.seed(seed1)\n",
        "    train = random.sample(list(range(1,n+1)),int(.5*n))\n",
        "\n",
        "    ## temp\n",
        "    remain = list(set(full_set)-set(train))\n",
        "\n",
        "    ## separate remaining into validation and test\n",
        "    random.seed(seed2)\n",
        "    valid = random.sample(remain,int(.5*len(remain)))\n",
        "    test = list(set(remain)-set(valid))\n",
        "    \n",
        "    return(train,valid,test)\n",
        "\n",
        "## gets file names for a particular type of trash, given indices\n",
        "    ## input: waste category and indices\n",
        "    ## output: file names \n",
        "def get_names(waste_type,indices):\n",
        "    file_names = [waste_type+str(i)+\".jpg\" for i in indices]\n",
        "    return(file_names)    \n",
        "\n",
        "## moves group of source files to another folder\n",
        "    ## input: list of source files and destination folder\n",
        "    ## no output\n",
        "def move_files(source_files,destination_folder):\n",
        "    for file in source_files:\n",
        "        shutil.move(file,destination_folder)"
      ],
      "metadata": {
        "id": "ffvciLRMSGLK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## paths will be train/cardboard, train/glass, etc...\n",
        "subsets = ['train','valid']\n",
        "waste_types = ['cardboard','glass','metal','paper','plastic','trash']\n",
        "\n",
        "## create destination folders for data subset and waste type\n",
        "for subset in subsets:\n",
        "    for waste_type in waste_types:\n",
        "        folder = os.path.join('data',subset,waste_type)\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "            \n",
        "if not os.path.exists(os.path.join('data','test')):\n",
        "    os.makedirs(os.path.join('data','test'))\n",
        "            \n",
        "## move files to destination folders for each waste type\n",
        "for waste_type in waste_types:\n",
        "    source_folder = os.path.join('dataset-resized',waste_type)\n",
        "    train_ind, valid_ind, test_ind = split_indices(source_folder,1,1)\n",
        "    \n",
        "    ## move source files to train\n",
        "    train_names = get_names(waste_type,train_ind)\n",
        "    train_source_files = [os.path.join(source_folder,name) for name in train_names]\n",
        "    train_dest = \"data/train/\"+waste_type\n",
        "    move_files(train_source_files,train_dest)\n",
        "    \n",
        "    ## move source files to valid\n",
        "    valid_names = get_names(waste_type,valid_ind)\n",
        "    valid_source_files = [os.path.join(source_folder,name) for name in valid_names]\n",
        "    valid_dest = \"data/valid/\"+waste_type\n",
        "    move_files(valid_source_files,valid_dest)\n",
        "    \n",
        "    ## move source files to test\n",
        "    test_names = get_names(waste_type,test_ind)\n",
        "    test_source_files = [os.path.join(source_folder,name) for name in test_names]\n",
        "    ## I use data/test here because the images can be mixed up\n",
        "    move_files(test_source_files,\"data/test\")"
      ],
      "metadata": {
        "id": "lhOQTUFcXcvj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## get a path to the folder with images\n",
        "path = Path(os.getcwd())/\"data\"\n",
        "path"
      ],
      "metadata": {
        "id": "tlhSE_ZfYAbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf7861c-1eeb-457e-991b-b79fe51f238c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/data')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://concordiauniversity.on.worldcat.org/search?queryString=Recyclable%20waste%20image%20recognition%20based%20on%20deep%20learning&databaseList= "
      ],
      "metadata": {
        "id": "ihgXxrXsiJkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfms = get_transforms(do_flip=True,flip_vert=True)\n",
        "data = ImageDataBunch.from_folder(path,test=\"test\",ds_tfms=tfms,bs=16)\n",
        "data"
      ],
      "metadata": {
        "id": "DDPy-0J8UbLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.classes)\n",
        "data.show_batch(rows=4,figsize=(10,8))\n"
      ],
      "metadata": {
        "id": "XhuVD3J3ZbYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn = create_cnn(data,models.resnet34,metrics=error_rate)"
      ],
      "metadata": {
        "id": "7QtLTBRCZxVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.model"
      ],
      "metadata": {
        "id": "vemYqbbSZx5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.lr_find(start_lr=1e-6,end_lr=1e1)\n",
        "learn.recorder.plot()"
      ],
      "metadata": {
        "id": "eJekED2nZ3Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.recorder.plot()"
      ],
      "metadata": {
        "id": "HNJSojBFZ34K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(2,max_lr=5.13e-03)"
      ],
      "metadata": {
        "id": "mOUBTr6OaWFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "losses,idxs = interp.top_losses()"
      ],
      "metadata": {
        "id": "kFP1hxW6aZ48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp.plot_top_losses(6, figsize=(15,11))"
      ],
      "metadata": {
        "id": "uTOPbSumhfTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc(interp.plot_top_losses)\n",
        "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
      ],
      "metadata": {
        "id": "0sSjX6WUhicA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interp.most_confused(min_val=2)"
      ],
      "metadata": {
        "id": "MUnx-quehoOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = learn.get_preds(ds_type=DatasetType.Test)"
      ],
      "metadata": {
        "id": "yKneHWMKhrkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.classes"
      ],
      "metadata": {
        "id": "p73xLaGih6gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## saves the index (0 to 5) of most likely (max) predicted class for each image\n",
        "max_idxs = np.asarray(np.argmax(preds[0],axis=1))"
      ],
      "metadata": {
        "id": "dY2Dfx4Uh9K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for max_idx in max_idxs:\n",
        "    result.append(data.classes[max_idx])"
      ],
      "metadata": {
        "id": "eJJhbhpliAo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "9nuLhbYjiD9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.data.test_ds[0][0]"
      ],
      "metadata": {
        "id": "b5IrII0GiHKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = []\n",
        "\n",
        "## convert POSIX paths to string first\n",
        "for label_path in data.test_ds.items:\n",
        "    y.append(str(label_path))\n",
        "    \n",
        "## then extract waste type from file path\n",
        "pattern = re.compile(\"([a-z]+)[0-9]+\")\n",
        "for i in range(len(y)):\n",
        "    y[i] = pattern.search(y[i]).group(1)"
      ],
      "metadata": {
        "id": "OwEug8NJiNc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## predicted values\n",
        "print(result[0:5])\n",
        "## actual values\n",
        "print(y[0:5])"
      ],
      "metadata": {
        "id": "opSsN2HTiRDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.data.test_ds[0][0]"
      ],
      "metadata": {
        "id": "ETtM8LpZiUeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y,result)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "wSu_ClugiXYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cm = pd.DataFrame(cm,waste_types,waste_types)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(df_cm,annot=True,fmt=\"d\",cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "UzTmapsLibZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "\n",
        "for r in range(len(cm)):\n",
        "    for c in range(len(cm)):\n",
        "        if (r==c):\n",
        "            correct += cm[r,c]"
      ],
      "metadata": {
        "id": "LXEE7cwyieda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = correct/sum(sum(cm))\n",
        "accuracy"
      ],
      "metadata": {
        "id": "uoSjZ6SQik77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}